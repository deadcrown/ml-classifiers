<h2>Data Preprocessing Notes</h2>
<hr>
<i>Ahmad Faiz</i>
<p>
    <strong>Removing data on Nan</strong><br>
    <code>df.dropna(axis=0)</code> drop rows with nulls<br>
    <code>df.dropna(axis=1)</code> drop columns with nulls<br>
    <code>df.dropna(thresh=k)</code> drop rows with nulls > k<br>
    <code>df.dropna(subset=[k])</code> drop rows by checking for nulls only in column k<br>
    <code>df.dropna(how='all')</code> drop rows with all nulls<br>
    <code>df.dropna()</code> defaults to axis=0(drop rows)
</p>
<p>
    <strong>Imputing data on Nan</strong><br>
    Using sklearn transformer Imputer<br>
    Imputer parameters: strategy=mean/median/most_frequent; axis=0(along columns)/1(along rows); missing_values='NaN'
    <br><code> 
        from sklearn.preprocessing import Imputer<br>
        imp = Imputer(strategy='mean', axis=0, missing_values='NaN')<br>
        X_train_imp = imp.fit_transform(X_train)<br>
        </code>
</p> 
<p>
    <strong>Handling categorical features</strong>
    <br><i><strong>Mapping Ordinal Features:</strong></i> Use df['ordinal_column'].map(map_dict) to map categorical 
    values to defined values relation of ordinal variable
    <br><i><strong>Mapping Class Labels:</strong></i>Use pandas map() or sklearn transformer LabelEncoder 
    Eg LabelEncoder().fit_transform(df['class_labels'].values)
    <br><i><strong>OneHotEncoding:</strong></i> To prevent false ordering between nominal features implement OneHotEncoding to create dummified variables for each value of the mapped nominal variable value.
    Use sklearn LabelEncoder first on the nominal variable followed by OneHotEncoder fit_transform.
    Alternatively use pandas.get_dummies()<br>
    <i>Example</i> 
    <br><code>OneHotEncoder(categorical_features=[0]).fit_transform(df.columns[:-1].values)</code>
    <br><code>pd.get_dummies(df[df.columns[:-1]]) </code>to dummify all string variables leaving all numeric intact
    <br><code>pd.get_dummies(df[df.columns[:-1]], drop_first=True)</code> to reduce correlation among dummified variables by dropping one feature value as (0,0) for other two is the same thing 
</p>
<p>
    <strong>Feature scaling</strong>
    <br>Decision trees are an exception to feature scaling, as we do axis aligned splits using KD Trees. 
    However for other algorithms(both parametric and non-parametric) if the position of data point in the feature space is of importance then feature scalign becomes cruial.
    Two common approcahes are <mark> Normalization</mark> and <mark>Standardization</mark>
    <br><strong>Normalization</strong> refers to min-max scaling which is <br><img src="images/norm.jpeg">
    <br>sklearn implementation needs a MinMaxScaler transformer 
    <br><code>
        from sklearn.preprocessing import MinMaxScaler<br>
        min_max_sclr = MinMaxScaler()<br>
        X_train_norm = min_max_sclr.fit_transform(X_train)<br>
        X_test_norm = min_max_sclr.transform(X_test)<br>
    </code>
    <br><strong>Standardization</strong> commonly implies taking mean standardization which is 
    <br><img src="images/mean_std.jpeg">
    <br>sklearn implementation needs a StandardScaler transformer<br>
    <code>
        from sklearn.preprocessing import StandardScaler<br>
        ssc = StandardScaler()<br>
        X_train_std = ssc.fit_transform(X_train)<br>
        X_test_std = ssc.transform(X_test)
    </code>
    <br>Usually standardization with mean centered at zero is better to learn weights tending towards zero as well.Also using standardization makes the data less sensitive to outliers and 
    also incorporates information about these outliers in train  
</p>
<p>
    <strong>Train Test Split</strong>
    <br>Can use sklearn model selection function train_test_split(from sklearn.model_selection import train_test_split)
    <br>parameters: train_test_split(X, Y, test_size, stratify, random_state)
    <br>test_size: test_split size; stratify=Y(class labels): to ensure that proportion of labels in train and test is same
    <br><code>
        from sklearn.model_selection import train_test_split<br>
        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, stratify=Y, random_state=1)<br>
        </code>
</p>
<p>
    <strong>Selecting important features</strong> 
    <br>This is important particularly in case of high variance(overfitting). One way to address variance is to reduce data dimensionality.
    Basically there are two main ways to reduce data dimensionality: <mark>Feature Selection</mark> and <mark>Feature Extraction</mark>
    <br><br>&bull;<mark><strong>Feature selection</strong></mark> refers to selecting a subset of features based on their importance in describing the test dataset. Methods which can be used are 
    <br> <i>Regularization parameter</i> variation for a paramteric model(eg Logistic Regression) and then plotting the weight coeffiecient distribution over features with increasing value of reg parameter
    <br><i>Random Forest</i> which tells the feature importance based on the contribution in reducing the impurity after the split. sklearn has a nice abstraction for getting feature importance .feature_importances_
    <br><br>
    L1 regularizer <br><img src="images/L1.jpeg">
    <br> L2 <br><img src="images/L2.jpeg"> 
    <br> Geometrically the optimization function is now bounded in the space defined by the L1(diamond) or L2(sphere). Since these are centered at origin 
    the optimization function makes a compromize on the gradient contour by digging into the hill limited by the regularizer. Increasing or decreasing the reg parameter(lambda) increases/decreases the radius of these restrictive spaces as the 
    optimization is more focused on minimizing the second term of the ERM equation. Hence model fit can be iterated with different values of reg parameter which will result in different weights assigned. This can now be plotted to see which feature contribute 
    to maximum weights on varying reg parameter. Example using Logistic regression<br>
    <br><code>
        for c in np.arange(-4., 6.)<br>
        &emsp;&emsp;LogisticRegression(penalty='L1', C=10**c)
    </code><br>
    <img src="images/L1featselect.jpeg" width="800">
    <br><br><strong>Random Forest</strong> for feature imprtance based on features which resulted in maximum loss of impurity in the dataset. In RF feature selection there is no assumption of linear decision boundary. 
    This measurement implies that features which result in average maximum decrease of impurity across all trees are ranked higher in terms of them alone explaining the train dataset.
    <br>scikit implementation<br>
    <code>
        from sklearn.ensemble import RandomForestClassifier<br>
        rf = RandomForestClassifier(criterion='entropy', n_estimators=1000, n_jobs=4)<br>
        rf.fit(X_train)<br>
        rf.feature_importances_<br>
    </code><br>
    sample plot for UCI wine<br>
    <img src="images/RF_feat.jpeg", width="800", height="400">
    <br><br><br>
    &bull;<mark><strong>Feature Extraction</strong></mark> implies taking the original dimensions and then projecting them to some lower dimension space to reduce dimensionality and also can be used for decision boundary viz purpose.
    PCA is one instance of feature extraction where the data is projected onto lower dimension space with the axes of this space being featues which explain the maximum variance for the class labels.
    <br>scikit implementation requires using a transformer PCA. After transformation use this dim space to train the model estimator<br>
    <code>
        from sklearn.model_decomposition import PCA<br>
        pca = PCA(n_components=2)<br>
        X_train_pca = pca.fit_transform(X_train)<br>
        X_test_pca = pca.transform(X_test)<br>
    </code>
</p>
